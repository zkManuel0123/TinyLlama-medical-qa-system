{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import Tool\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.llms.base import LLM\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Any, List, Optional\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel, PeftConfig\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.cuda\n",
    "from torch.amp import autocast\n",
    "import gc\n",
    "import faiss \n",
    "import pickle\n",
    "import litellm\n",
    "from typing import Any, List, Optional, Dict\n",
    "from pydantic import Field, BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface token\n",
    "def setup_huggingface(token):\n",
    "    login(token)\n",
    "    os.environ[\"HUGGINGFACE_TOKEN\"] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_fine_tuned_model(\n",
    "    base_model_path=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    lora_model_path=\"Kai0123/tinyllama-medical-qa-lora-model\"\n",
    "):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        \n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    \n",
    "    config = PeftConfig.from_pretrained(lora_model_path)\n",
    "    \n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "        # load_in_8bit=False,  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, lora_model_path).to(device)\n",
    "\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, index_path=\"faiss_index/docs.index\", data_path=\"faiss_index/texts.pkl\", \n",
    "                 model_name=\"ncbi/MedCPT-Query-Encoder\"):\n",
    "        \n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        \n",
    "        with open(data_path, \"rb\") as f:\n",
    "            stored_data = pickle.load(f)\n",
    "            self.texts = stored_data[\"texts\"]\n",
    "            self.ids = stored_data[\"ids\"]\n",
    "            \n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "    def compute_embedding(self, query):\n",
    "        inputs = self.tokenizer([query], padding=True, truncation=True, \n",
    "                              max_length=512, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        return embedding\n",
    "        \n",
    "    def similarity_search(self, query, k=3):\n",
    "        \n",
    "        query_embedding = self.compute_embedding(query)\n",
    "        \n",
    "        \n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            results.append({\n",
    "                'text': self.texts[idx],\n",
    "                'id': self.ids[idx],\n",
    "                'score': 1 - distances[0][i]  \n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersLLM(LLM, BaseModel):\n",
    "    base_model_path: str = Field(..., description=\"Local path to the base model\")\n",
    "    lora_model_path: str = Field(..., description=\"Local path to the LoRA model\")\n",
    "    temperature: float = Field(default=0.7, description=\"Sampling temperature\")\n",
    "    max_tokens: int = Field(default=512, description=\"Maximum number of tokens to generate\")\n",
    "    \n",
    "    model: Any = None\n",
    "    tokenizer: Any = None\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"Loading model and tokenizer...\")\n",
    "        try:\n",
    "            # 确保路径存在\n",
    "            if not os.path.exists(self.base_model_path):\n",
    "                raise ValueError(f\"Base model path does not exist: {self.base_model_path}\")\n",
    "            if not os.path.exists(self.lora_model_path):\n",
    "                raise ValueError(f\"LoRA model path does not exist: {self.lora_model_path}\")\n",
    "                \n",
    "            print(f\"Loading tokenizer from {self.base_model_path}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.base_model_path,\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Loading base model from {self.base_model_path}\")\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.base_model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Loading LoRA model from {self.lora_model_path}\")\n",
    "            config = PeftConfig.from_pretrained(self.lora_model_path)\n",
    "            \n",
    "            lora_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                self.lora_model_path,\n",
    "                config=config,\n",
    "                is_trainable=False,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            print(\"Merging LoRA weights...\")\n",
    "            self.model = lora_model.merge_and_unload()\n",
    "            \n",
    "            print(\"Model loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        try:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error in model inference: {str(e)}\")\n",
    "            return \"Error generating response\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "        \n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"base_model_path\": self.base_model_path,\n",
    "            \"lora_model_path\": self.lora_model_path,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_agents(vectorstore):\n",
    "\n",
    "    from langchain.agents import AgentType, initialize_agent\n",
    "    from langchain.agents import Tool\n",
    "    from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "    \n",
    "    base_model_path = \"./models/TinyLlama-1.1B-Chat-v1.0\"  \n",
    "    lora_model_path = \"./models/tinyllama-medical-qa-lora-model\" \n",
    "\n",
    "    print(\"Initializing custom LLM...\")\n",
    "    llm = TransformersLLM(\n",
    "        base_model_path=base_model_path,\n",
    "        lora_model_path=lora_model_path,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # print(\"Initializing custom LLM...\")\n",
    "    # custom_llm = TransformersLLM(\n",
    "    #     base_model_path=base_model_path,\n",
    "    #     lora_model_path=lora_model_path,\n",
    "    #     temperature=0.7,\n",
    "    #     max_tokens=512\n",
    "    # )\n",
    "    \n",
    "    # search_tool = Tool(\n",
    "    #     name='search_medical_literature',\n",
    "    #     description='Search through medical literature database',\n",
    "    #     func=vectorstore.similarity_search\n",
    "    # )\n",
    "    \n",
    "    tools = [\n",
    "        Tool(\n",
    "            name='search_medical_literature',\n",
    "            func=vectorstore.similarity_search,\n",
    "            description='Search through medical literature database'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    \n",
    "    retriever_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "    analyzer_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "    generator_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # retriever_agent = Agent(\n",
    "    #     name='Retriever',\n",
    "    #     role='Expert responsible for retrieving relevant information from the knowledge base',\n",
    "    #     goal='Find the most relevant medical literature for the query',\n",
    "    #     backstory='I am an AI assistant specializing in medical literature retrieval',\n",
    "    #     tools=[search_tool],\n",
    "    #     llm=custom_llm,\n",
    "    #     verbose=True\n",
    "    # )\n",
    "\n",
    "    \n",
    "    # analyzer_agent = Agent(\n",
    "    #     name='Analyzer',\n",
    "    #     role='Medical information analysis expert',\n",
    "    #     goal='Analyze the retrieved information and extract key information',\n",
    "    #     backstory='I specialize in analyzing medical literature and extracting important information',\n",
    "    #     llm=custom_llm,\n",
    "    #     verbose=True\n",
    "    # )\n",
    "\n",
    "    \n",
    "    # generator_agent = Agent(\n",
    "    #     name='Generator',\n",
    "    #     role='Medical Answer Generation Expert',\n",
    "    #     goal='Generate accurate answers to medical questions based on analysis results',\n",
    "    #     backstory='I am responsible for translating the analyzed information into clear answers',\n",
    "    #     llm=custom_llm,\n",
    "    #     verbose=True\n",
    "    # )\n",
    "\n",
    "    \n",
    "    retriever_agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        memory=retriever_memory,\n",
    "        verbose=True,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": \"You are an expert responsible for retrieving relevant information from the medical knowledge base.\",\n",
    "            \"ai_prefix\": \"Retriever\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    analyzer_agent = initialize_agent(\n",
    "        tools=[],  \n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        memory=analyzer_memory,\n",
    "        verbose=True,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": \"You are a medical information analysis expert.\",\n",
    "            \"ai_prefix\": \"Analyzer\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    generator_agent = initialize_agent(\n",
    "        tools=[],  \n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        memory=generator_memory,\n",
    "        verbose=True,\n",
    "        agent_kwargs={\n",
    "            \"prefix\": \"You are an expert in generating accurate medical answers based on analysis results.\",\n",
    "            \"ai_prefix\": \"Generator\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    return retriever_agent, analyzer_agent, generator_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tasks(retriever_agent, analyzer_agent, generator_agent, query):\n",
    "    task1 = Task(\n",
    "        description=f\"Search medical literature related to the question: {query}\",\n",
    "        expected_output=\"A list of relevant medical literature references related to the query\",\n",
    "        agent=retriever_agent\n",
    "    )\n",
    "    \n",
    "    task2 = Task(\n",
    "        description=\"Analyze the retrieved literature and extract key information\",\n",
    "        expected_output=\"A summary of key findings and important information from the literature\",\n",
    "        agent=analyzer_agent\n",
    "    )\n",
    "    \n",
    "    task3 = Task(\n",
    "        description=\"Generate the final answer\",\n",
    "        expected_output=\"A concise and accurate answer to the medical question based on the analyzed information\",\n",
    "        agent=generator_agent\n",
    "    )\n",
    "    \n",
    "    return [task1, task2, task3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model and vectorstore...\n",
      "\n",
      "Creating agents...\n",
      "Initializing custom LLM...\n",
      "Loading model and tokenizer...\n",
      "Loading tokenizer from ./models/TinyLlama-1.1B-Chat-v1.0\n",
      "Loading base model from ./models/TinyLlama-1.1B-Chat-v1.0\n",
      "Loading LoRA model from ./models/tinyllama-medical-qa-lora-model\n",
      "Merging LoRA weights...\n",
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manue\\AppData\\Local\\Temp\\ipykernel_26144\\2310932477.py:44: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  retriever_memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
      "C:\\Users\\manue\\AppData\\Local\\Temp\\ipykernel_26144\\2310932477.py:82: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  retriever_agent = initialize_agent(\n",
      "C:\\Users\\manue\\AppData\\Local\\Temp\\ipykernel_26144\\939444595.py:22: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieval_result = retriever_agent.run(f\"Find relevant information about: {query}\")\n",
      "c:\\Users\\manue\\anaconda3\\envs\\crewai\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\anaconda3\\envs\\crewai\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:602: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mYou are an expert responsible for retrieving relevant information from the medical knowledge base.\n",
      "\n",
      "> search_medical_literature: Search through medical literature database\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of [search_medical_literature]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "Retriever: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Find relevant information about: What are the common treatments for diabetes?\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Search medical literature database.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\anaconda3\\envs\\crewai\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mYou are a medical information analysis expert.\n",
      "\n",
      "\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of []\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "Analyzer: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Find relevant information about: What are the common treatments for diabetes?\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Search medical literature database.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Use PubMed search engine.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\anaconda3\\envs\\crewai\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mYou are an expert in generating accurate medical answers based on analysis results.\n",
      "\n",
      "\n",
      "\n",
      "To use a tool, please use the following format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: the action to take, should be one of []\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "```\n",
      "\n",
      "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
      "\n",
      "```\n",
      "Thought: Do I need to use a tool? No\n",
      "Generator: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Generate a clear answer based on this analysis: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Find relevant information about: What are the common treatments for diabetes?\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Search medical literature database.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Use PubMed search engine.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Use the American Diabetes Association's website.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Final Answer: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Generate a clear answer based on this analysis: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Find relevant information about: What are the common treatments for diabetes?\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Search medical literature database.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Use PubMed search engine.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\n",
      "\n",
      "Previous conversation history:\n",
      "\n",
      "\n",
      "New input: Analyze this medical information: [your response here]\n",
      "```\n",
      "\n",
      "Begin!\n",
      "\n",
      "Previous response: Yes, I need to use a tool.\n",
      "\n",
      "New action: Use the American Diabetes Association's website.\n",
      "\n",
      "Observation: The result is not relevant.\n",
      "\n",
      "Previous response: No, I do not need to use a tool.\n",
      "```\n",
      "\n",
      "End!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "        \n",
    "        HUGGINGFACE_TOKEN = \"your huggingface tocken\"\n",
    "        setup_huggingface(HUGGINGFACE_TOKEN)\n",
    "        \n",
    "        \n",
    "        print(\"\\nLoading model and vectorstore...\")\n",
    "        # model, tokenizer = load_fine_tuned_model()\n",
    "        vectorstore = VectorStore()\n",
    "        \n",
    "        \n",
    "        print(\"\\nCreating agents...\")\n",
    "        retriever_agent, analyzer_agent, generator_agent = create_agents(vectorstore)\n",
    "        \n",
    "        \n",
    "        print(\"\\nProcessing query...\")\n",
    "        query = \"What are the common treatments for diabetes?\"\n",
    "\n",
    "\n",
    "        #\n",
    "        retrieval_result = retriever_agent.run(f\"Find relevant information about: {query}\")\n",
    "        analysis_result = analyzer_agent.run(f\"Analyze this medical information: {retrieval_result}\")\n",
    "        final_answer = generator_agent.run(f\"Generate a clear answer based on this analysis: {analysis_result}\")\n",
    "    \n",
    "        print(\"\\nFinal Answer:\", final_answer)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
