{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: 导入必要的库\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 加载数据集并取前1000行\n",
    "def load_and_prepare_data():\n",
    "    dataset = load_dataset(\"enelpol/rag-mini-bioasq\", \"text-corpus\")\n",
    "    # 只取test split的前1000行\n",
    "    texts = dataset['test']['passage'][:1000]\n",
    "    ids = dataset['test']['id'][:1000]\n",
    "    return texts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 加载模型和tokenizer\n",
    "def load_model_and_tokenizer():\n",
    "    model_name = \"ncbi/MedCPT-Query-Encoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    # 如果有GPU则使用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 计算embeddings的函数\n",
    "def compute_embeddings(texts, model, tokenizer, device, batch_size=32):\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # 对文本进行编码\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                         max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        # 将输入移到相应设备\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # 计算embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # 使用最后一层的[CLS]标记的输出作为文档的嵌入\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # 将所有batch的embeddings合并\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 创建和保存FAISS索引\n",
    "def create_and_save_faiss_index(embeddings, texts, ids, save_dir=\"faiss_index\"):\n",
    "    # 创建维度相同的FAISS索引\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    \n",
    "    # 将向量添加到索引中\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 保存FAISS索引\n",
    "    faiss.write_index(index, os.path.join(save_dir, \"docs.index\"))\n",
    "    \n",
    "    # 保存文本和ID映射\n",
    "    with open(os.path.join(save_dir, \"texts.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\"texts\": texts, \"ids\": ids}, f)\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 主函数调用所有步骤\n",
    "def main():\n",
    "    # 1. 加载数据\n",
    "    texts, ids = load_and_prepare_data()\n",
    "    print(f\"Loaded {len(texts)} documents\")\n",
    "    \n",
    "    # 2. 加载模型和tokenizer\n",
    "    model, tokenizer, device = load_model_and_tokenizer()\n",
    "    print(f\"Model loaded and moved to {device}\")\n",
    "    \n",
    "    # 3. 计算embeddings\n",
    "    embeddings = compute_embeddings(texts, model, tokenizer, device)\n",
    "    print(f\"Computed embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # 4. 创建和保存FAISS索引\n",
    "    index = create_and_save_faiss_index(embeddings, texts, ids)\n",
    "    print(f\"Created and saved FAISS index with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据示例:\n",
      "['New data on viruses isolated from patients with subacute thyroiditis de Quervain \\nare reported. Characteristic morphological, cytological, some physico-chemical \\nand biological features of the isolated viruses are described. A possible role \\nof these viruses in human and animal health disorders is discussed. The isolated \\nviruses remain unclassified so far.', \"We describe an improved method for detecting deficiency of the acid hydrolase, \\nalpha-1,4-glucosidase in leukocytes, the enzyme defect in glycogen storage \\ndisease Type II (Pompe disease). The procedure requires smaller volumes of blood \\nand less time than previous methods. The assay involves the separation of \\nleukocytes by Peter's method for beta-glucosidase and a modification of Salafsky \\nand Nadler's fluorometric method for alpha-glucosidase.\"]\n",
      "\n",
      "ID示例:\n",
      "[9797, 11906]\n"
     ]
    }
   ],
   "source": [
    "# 测试数据加载\n",
    "texts, ids = load_and_prepare_data()\n",
    "print(f\"数据示例:\\n{texts[:2]}\\n\")\n",
    "print(f\"ID示例:\\n{ids[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 测试模型加载\n",
    "model, tokenizer, device = load_model_and_tokenizer()\n",
    "print(f\"模型设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试embeddings形状: (5, 768)\n"
     ]
    }
   ],
   "source": [
    "# 测试小批量数据的embeddings计算\n",
    "test_embeddings = compute_embeddings(texts[:5], model, tokenizer, device)\n",
    "print(f\"测试embeddings形状: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 documents\n",
      "Model loaded and moved to cuda\n",
      "Computed embeddings with shape: (1000, 768)\n",
      "Created and saved FAISS index with 1000 vectors\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询: What is Pompe disease?\n",
      "\n",
      "最相关的文档:\n",
      "\n",
      "--- 相似度得分: -61.9027 ---\n",
      "ID: 1294876\n",
      "文本: The mucopolysaccharidosis represent a broad spectrum of disorders due to the \n",
      "deficiency of one of a group of enzymes which degrade three classes of \n",
      "mucopolysaccharides: heparan sulfate, dermatan-sul...\n",
      "\n",
      "--- 相似度得分: -62.4059 ---\n",
      "ID: 1588015\n",
      "文本: A sister and brother, now aged 7 and 9 years, presented with developmental \n",
      "arrest, gait disturbance, dementia, and a progressive myoclonic epilepsy \n",
      "syndrome with hyperacusis in the second year of li...\n",
      "\n",
      "--- 相似度得分: -69.7647 ---\n",
      "ID: 2079710\n",
      "文本: The inherited deficiency of galactosylceramide beta-galactosidase (E.C. \n",
      "3.2.1.46: galactocerebrosidase) activity results in globoid cell leukodystrophy \n",
      "in humans (Krabbe disease) and in mice (twitch...\n"
     ]
    }
   ],
   "source": [
    "# 测试加载和查询\n",
    "def test_vectorstore():\n",
    "    # 1. 加载保存的索引\n",
    "    index = faiss.read_index(\"faiss_index/docs.index\")\n",
    "    \n",
    "    # 2. 加载保存的文本和ID映射\n",
    "    with open(\"faiss_index/texts.pkl\", \"rb\") as f:\n",
    "        stored_data = pickle.load(f)\n",
    "        texts = stored_data[\"texts\"]\n",
    "        ids = stored_data[\"ids\"]\n",
    "    \n",
    "    # 3. 加载模型来编码查询\n",
    "    model, tokenizer, device = load_model_and_tokenizer()\n",
    "    \n",
    "    # 4. 测试查询\n",
    "    test_query = \"What is Pompe disease?\"\n",
    "    # 使用相同的方式编码查询\n",
    "    query_embedding = compute_embeddings([test_query], model, tokenizer, device)\n",
    "    \n",
    "    # 5. 搜索最相关的3个文档\n",
    "    k = 3  # 返回前3个最相似的文档\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    print(\"查询:\", test_query)\n",
    "    print(\"\\n最相关的文档:\")\n",
    "    for i in range(k):\n",
    "        print(f\"\\n--- 相似度得分: {1 - distances[0][i]:.4f} ---\")\n",
    "        print(f\"ID: {ids[indices[0][i]]}\")\n",
    "        print(f\"文本: {texts[indices[0][i]][:200]}...\")\n",
    "\n",
    "# 运行测试\n",
    "test_vectorstore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
